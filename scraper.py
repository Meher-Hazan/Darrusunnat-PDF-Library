import os
import json
import asyncio
import re
import random
from telethon import TelegramClient
from telethon.sessions import StringSession
from PIL import Image, ImageDraw

# --- CONFIGURATION ---
API_ID = int(os.environ.get('API_ID', 0))
API_HASH = os.environ.get('API_HASH', '')
SESSION_STRING = os.environ.get('SESSION_STRING', '')
CHANNEL_ID = int(os.environ.get('CHANNEL_ID', 0))

DATA_FILE = 'books_data.json'
IMAGES_DIR = 'images'

if not os.path.exists(IMAGES_DIR):
    os.makedirs(IMAGES_DIR)

# --- üß† LAYER 1: DIRECT KNOWLEDGE BASE ---
# The robot checks this list FIRST. If it finds these words, it knows the writer instantly.
AI_KNOWLEDGE = {
    # HADITH & TAFSIR
    'bukhari': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶¨‡ßÅ‡¶ñ‡¶æ‡¶∞‡ßÄ (‡¶∞‡¶π.)',
    'muslim': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ (‡¶∞‡¶π.)',
    'tirmidhi': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶§‡¶ø‡¶∞‡¶Æ‡¶ø‡¶Ø‡ßÄ (‡¶∞‡¶π.)',
    'nasai': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶®‡¶æ‡¶∏‡¶æ‡¶à (‡¶∞‡¶π.)',
    'abu daud': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶Ü‡¶¨‡ßÅ ‡¶¶‡¶æ‡¶â‡¶¶ (‡¶∞‡¶π.)',
    'ibn majah': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶á‡¶¨‡¶®‡ßá ‡¶Æ‡¶æ‡¶ú‡¶æ‡¶π (‡¶∞‡¶π.)',
    'ryadus': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶®‡¶¨‡¶¨‡ßÄ (‡¶∞‡¶π.)',
    'riyadus': '‡¶á‡¶Æ‡¶æ‡¶Æ ‡¶®‡¶¨‡¶¨‡ßÄ (‡¶∞‡¶π.)',
    'mishkat': '‡¶ì‡ßü‡¶æ‡¶≤‡ßÄ‡¶â‡¶¶‡ßç‡¶¶‡ßÄ‡¶® ‡¶Ü‡¶≤-‡¶ñ‡¶æ‡¶§‡ßÄ‡¶¨ (‡¶∞‡¶π.)',
    'ibn kathir': '‡¶π‡¶æ‡¶´‡ßá‡¶ú ‡¶á‡¶¨‡¶®‡ßá ‡¶ï‡¶æ‡¶∏‡ßÄ‡¶∞ (‡¶∞‡¶π.)',
    'jalalain': '‡¶ú‡¶æ‡¶≤‡¶æ‡¶≤‡ßÅ‡¶¶‡ßç‡¶¶‡¶ø‡¶® ‡¶∏‡ßÅ‡¶Ø‡¶º‡ßÅ‡¶§‡ßÄ (‡¶∞‡¶π.)',
    'mareful': '‡¶Æ‡ßÅ‡¶´‡¶§‡¶ø ‡¶∂‡¶´‡¶ø ‡¶â‡¶∏‡¶Æ‡¶æ‡¶®‡ßÄ (‡¶∞‡¶π.)',
    'fi zilalil': '‡¶∏‡¶æ‡¶á‡ßü‡ßá‡¶¶ ‡¶ï‡ßÅ‡¶§‡ßÅ‡¶¨ (‡¶∞‡¶π.)',
    'tafhimul': '‡¶∏‡¶æ‡¶á‡ßü‡ßá‡¶¶ ‡¶Ü‡¶¨‡ßÅ‡¶≤ ‡¶Ü\'‡¶≤‡¶æ ‡¶Æ‡¶ì‡¶¶‡ßÅ‡¶¶‡ßÄ (‡¶∞‡¶π.)',
    
    # POPULAR WRITERS
    'ariff azad': '‡¶Ü‡¶∞‡¶ø‡¶´ ‡¶Ü‡¶ú‡¶æ‡¶¶',
    'paradoxical': '‡¶Ü‡¶∞‡¶ø‡¶´ ‡¶Ü‡¶ú‡¶æ‡¶¶',
    'bela furabar': '‡¶Ü‡¶∞‡¶ø‡¶´ ‡¶Ü‡¶ú‡¶æ‡¶¶',
    'mizanur rahman': '‡¶Æ‡¶ø‡¶ú‡¶æ‡¶®‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶Ü‡¶ú‡¶π‡¶æ‡¶∞‡ßÄ',
    'azhari': '‡¶Æ‡¶ø‡¶ú‡¶æ‡¶®‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶Ü‡¶ú‡¶π‡¶æ‡¶∞‡ßÄ',
    'ahmadullah': '‡¶∂‡¶æ‡ßü‡¶ñ ‡¶Ü‡¶π‡¶Æ‡¶æ‡¶¶‡ßÅ‡¶≤‡ßç‡¶≤‡¶æ‡¶π',
    'nasiruddin': '‡¶®‡¶æ‡¶∏‡¶ø‡¶∞‡ßÅ‡¶¶‡ßç‡¶¶‡¶ø‡¶® ‡¶Ü‡¶≤‡¶¨‡¶æ‡¶®‡ßÄ (‡¶∞‡¶π.)',
    'albani': '‡¶®‡¶æ‡¶∏‡¶ø‡¶∞‡ßÅ‡¶¶‡ßç‡¶¶‡¶ø‡¶® ‡¶Ü‡¶≤‡¶¨‡¶æ‡¶®‡ßÄ (‡¶∞‡¶π.)',
    'zakariya': '‡¶∂‡¶æ‡ßü‡¶ñ ‡¶ú‡¶æ‡¶ï‡¶æ‡¶∞‡¶ø‡ßü‡¶æ (‡¶∞‡¶π.)',
    'iqbal': '‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶Æ‡¶æ ‡¶á‡¶ï‡¶¨‡¶æ‡¶≤',
    'rahe belayat': '‡¶°. ‡¶ñ‡¶®‡ßç‡¶¶‡¶ï‡¶æ‡¶∞ ‡¶Ü‡¶¨‡ßç‡¶¶‡ßÅ‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶ú‡¶æ‡¶π‡¶æ‡¶ô‡ßç‡¶ó‡ßÄ‡¶∞',
    'jannat': '‡¶°. ‡¶ñ‡¶®‡ßç‡¶¶‡¶ï‡¶æ‡¶∞ ‡¶Ü‡¶¨‡ßç‡¶¶‡ßÅ‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶ú‡¶æ‡¶π‡¶æ‡¶ô‡ßç‡¶ó‡ßÄ‡¶∞',
    'himu': '‡¶π‡ßÅ‡¶Æ‡¶æ‡ßü‡ßÇ‡¶® ‡¶Ü‡¶π‡¶Æ‡ßá‡¶¶',
    'misir ali': '‡¶π‡ßÅ‡¶Æ‡¶æ‡ßü‡ßÇ‡¶® ‡¶Ü‡¶π‡¶Æ‡ßá‡¶¶',
    'sharat': '‡¶∂‡¶∞‡ßé‡¶ö‡¶®‡ßç‡¶¶‡ßç‡¶∞ ‡¶ö‡¶ü‡ßç‡¶ü‡ßã‡¶™‡¶æ‡¶ß‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º',
    'rabindra': '‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞'
}

# --- üß† LAYER 2: HONORIFICS (Smart Guessing) ---
# If the robot sees these titles in a name, it assumes it is a writer.
HONORIFICS = [
    'dr.', 'dr ', 'prof.', 'sheikh', 'shaykh', 'imam', 'mufti', 'maulana', 
    'moulana', 'allama', 'hafez', 'qari', 'ustadh', 'writer', 'author',
    '‡¶°.', '‡¶Ö‡¶ß‡ßç‡¶Ø‡¶æ‡¶™‡¶ï', '‡¶∂‡¶æ‡ßü‡¶ñ', '‡¶á‡¶Æ‡¶æ‡¶Æ', '‡¶Æ‡ßÅ‡¶´‡¶§‡¶ø', '‡¶Æ‡¶æ‡¶ì‡¶≤‡¶æ‡¶®‡¶æ', '‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶Æ‡¶æ', '‡¶π‡¶æ‡¶´‡ßá‡¶ú'
]

# --- CATEGORY RULES ---
CATEGORIES = {
    'hadith': ['hadith', 'bukhari', 'muslim', '‡¶π‡¶æ‡¶¶‡¶ø‡¶∏', '‡¶¨‡ßÅ‡¶ñ‡¶æ‡¶∞‡ßÄ', '‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ', '‡¶§‡¶ø‡¶∞‡¶Æ‡¶ø‡¶Ø‡ßÄ', '‡¶∏‡ßÅ‡¶®‡¶æ‡¶®'],
    'aqeedah': ['aqeedah', 'tawheed', 'iman', 'shirk', '‡¶Ü‡¶ï‡¶ø‡¶¶‡¶æ', '‡¶à‡¶Æ‡¶æ‡¶®', '‡¶§‡¶æ‡¶ì‡¶π‡ßÄ‡¶¶', '‡¶∂‡¶ø‡¶∞‡¶ï'],
    'fiqh': ['fiqh', 'salah', 'namaz', 'zakat', 'hajj', '‡¶´‡¶ø‡¶ï‡¶π', '‡¶®‡¶æ‡¶Æ‡¶æ‡¶ú', '‡¶∞‡ßã‡¶ú‡¶æ', '‡¶´‡¶§‡ßã‡ßü‡¶æ', '‡¶Æ‡¶æ‡¶∏‡¶æ‡¶Ø‡¶º‡ßá‡¶≤'],
    'history': ['history', 'seerah', 'biography', 'battle', '‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏', '‡¶∏‡¶ø‡¶∞‡¶æ‡¶§', '‡¶ú‡ßÄ‡¶¨‡¶®‡ßÄ', '‡¶Ø‡ßÅ‡¶¶‡ßç‡¶ß', '‡¶ñ‡ßá‡¶≤‡¶æ‡¶´‡¶§'],
    'quran': ['quran', 'tafsir', 'tajweed', 'ayat', '‡¶ï‡ßÅ‡¶∞‡¶Ü‡¶®', '‡¶§‡¶æ‡¶´‡¶∏‡¶ø‡¶∞', '‡¶§‡¶æ‡¶ú‡¶¨‡ßÄ‡¶¶', '‡¶∏‡ßÅ‡¶∞‡¶æ'],
    'novel': ['novel', 'story', '‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏', '‡¶ó‡¶≤‡ßç‡¶™', '‡¶ï‡¶æ‡¶π‡¶ø‡¶®‡¶ø', '‡¶≠‡ßç‡¶∞‡¶Æ‡¶£', '‡¶∏‡¶Æ‡¶ó‡ßç‡¶∞', '‡¶®‡¶æ‡¶ü‡¶ï'],
    'dua': ['dua', 'zikr', 'azkar', 'munajat', '‡¶¶‡ßã‡ßü‡¶æ', '‡¶ú‡¶ø‡¶ï‡¶ø‡¶∞', '‡¶Ü‡¶Æ‡¶≤', '‡¶Æ‡ßÅ‡¶®‡¶æ‡¶ú‡¶æ‡¶§']
}

def clean_text(text):
    """Deep cleaning of filenames"""
    if not text: return ""
    text = str(text)
    text = os.path.splitext(text)[0] # Remove .pdf
    # Remove things like "01. ", "02-", "[PDF]", website links
    text = re.sub(r'^[\d\.\-\_\(\)\[\]\s]+', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)
    text = re.sub(r'www\.[a-zA-Z0-9-]+\.[a-z]+', '', text)
    return text.strip()

def detect_writer_smart(title, raw_text=""):
    """
    THE SUPER BRAIN üß†
    1. Checks Knowledge Base.
    2. Checks Pattern Matching (Separators).
    3. Checks Honorifics.
    """
    search_text = (title + " " + raw_text).lower()
    
    # 1. Check AI Knowledge Base
    for keyword, writer in AI_KNOWLEDGE.items():
        if keyword in search_text:
            return writer

    # 2. Try Pattern Matching (Splitting by ' - ' or ' | ')
    # Looks for: "Book Name - Writer Name"
    separators = [r'\s+-\s+', r'\s+\|\s+', r'\s+‚Äì\s+', r'\s+by\s+', r'\s+_\s+']
    for sep in separators:
        parts = re.split(sep, title, 1)
        if len(parts) == 2:
            part1 = parts[0].strip()
            part2 = parts[1].strip()
            
            # Sub-logic: Which part is the writer?
            # If Part 2 has an honorific, it's the writer.
            if any(h in part2.lower() for h in HONORIFICS):
                return part2
            # If Part 1 has an honorific, it's the writer (Rare: "Imam Bukhari - Sahih")
            if any(h in part1.lower() for h in HONORIFICS):
                return part1
            
            # If no honorific, assume Part 2 is writer if it's short enough
            if len(part2) < 40 and not re.search(r'\d', part2):
                return part2

    return "‡¶Ö‡¶ú‡ßç‡¶û‡¶æ‡¶§" # Bangla for Unknown

def detect_category(text):
    text = text.lower()
    for cat, keywords in CATEGORIES.items():
        if any(k in text for k in keywords):
            return cat.capitalize()
    return "General"

def generate_cover(book_id):
    try:
        width, height = 400, 600
        color = (15, 76, 58)
        img = Image.new('RGB', (width, height), color=color)
        d = ImageDraw.Draw(img)
        d.rectangle([20, 20, width-20, height-20], outline="#FFD700", width=5)
        filename = f"{book_id}_gen.jpg"
        path = os.path.join(IMAGES_DIR, filename)
        img.save(path)
        return f"images/{filename}"
    except:
        return ""

async def main():
    print("--- ü§ñ STARTING INTELLIGENT ROBOT ---")
    
    try:
        client = TelegramClient(StringSession(SESSION_STRING), API_ID, API_HASH)
        await client.start()
    except Exception as e:
        print(f"Login Error: {e}")
        return

    # 1. LOAD EXISTING DATABASE
    all_books = []
    existing_ids = set()
    
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                all_books = json.load(f)
                for b in all_books:
                    existing_ids.add(b['id'])
        except: pass

    # 2. SCAN FOR NEW BOOKS (Last 200)
    print("Scanning Telegram for NEW books...")
    messages = await client.get_messages(CHANNEL_ID, limit=200)
    new_books_count = 0
    
    for message in reversed(messages):
        if message.id in existing_ids: continue

        if message.document and message.document.mime_type == 'application/pdf':
            # Get Name
            raw_name = ""
            if message.file and message.file.name: raw_name = message.file.name
            elif message.text: raw_name = message.text.split('\n')[0]
            
            if not raw_name: continue
            
            title = clean_text(raw_name)
            caption = message.text or ""
            
            # Intelligent Detection
            author = detect_writer_smart(title, caption)
            category = detect_category(title + " " + caption)
            
            # Cover
            cover_path = generate_cover(message.id)
            
            # Link
            clean_chan_id = str(CHANNEL_ID).replace("-100", "")
            link = f"https://t.me/c/{clean_chan_id}/{message.id}"

            book = {
                "id": message.id,
                "title": title,
                "author": author,
                "category": category,
                "link": link,
                "image": cover_path
            }
            
            all_books.append(book)
            existing_ids.add(message.id)
            new_books_count += 1
            print(f" + New: {title} | {author}")

    # 3. RE-SCAN OLD BOOKS (The Fix)
    # The robot now checks every single book in your database to see if it can fix "Unknown" authors
    print("Re-scanning OLD books for missing authors...")
    fixed_count = 0
    
    for book in all_books:
        # If author is missing, empty, or 'Unknown'/'‡¶Ö‡¶ú‡ßç‡¶û‡¶æ‡¶§'
        if not book.get('author') or book['author'] in ["‡¶Ö‡¶ú‡ßç‡¶û‡¶æ‡¶§", "Unknown", "", "‡¶Ö‡¶ú‡ßç‡¶û‡¶æ‡¶§ ‡¶≤‡ßá‡¶ñ‡¶ï"]:
            
            # Try to detect again using the smart logic on the title
            new_author = detect_writer_smart(book['title'])
            
            if new_author != "‡¶Ö‡¶ú‡ßç‡¶û‡¶æ‡¶§":
                book['author'] = new_author
                fixed_count += 1
                # Also clean the title (remove the author name from title if it was found there)
                # This keeps titles clean: "Sajid - Arif Azad" -> Title: "Sajid", Author: "Arif Azad"
                if new_author in book['title']:
                    book['title'] = book['title'].replace(new_author, "").replace("-", "").replace("|", "").strip()
                
                print(f" üõ† Fixed: {book['title']} | ‚úçÔ∏è {new_author}")

    # 4. SAVE EVERYTHING
    if new_books_count > 0 or fixed_count > 0:
        all_books.sort(key=lambda x: x['id'], reverse=True)
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_books, f, indent=4, ensure_ascii=False)
        print(f"--- ‚úÖ DONE: Added {new_books_count} new, Fixed {fixed_count} old ---")
    else:
        print("--- Database up to date ---")

if __name__ == '__main__':
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(main())
